{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8tQJd2YSCfWR"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "D7tqLMoKF6uq"
   },
   "source": [
    "Deep Learning\n",
    "=============\n",
    "\n",
    "Assignment 6\n",
    "------------\n",
    "\n",
    "After training a skip-gram model in `5_word2vec.ipynb`, the goal of this notebook is to train a LSTM character model over [Text8](http://mattmahoney.net/dc/textdata) data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "MvEblsgEXxrd"
   },
   "outputs": [],
   "source": [
    "# These are all the modules we'll be using later. Make sure you can import them\n",
    "# before proceeding further.\n",
    "from __future__ import print_function\n",
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "import string\n",
    "import tensorflow as tf\n",
    "import zipfile\n",
    "from six.moves import range\n",
    "from six.moves.urllib.request import urlretrieve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 5993,
     "status": "ok",
     "timestamp": 1445965582896,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "RJ-o3UBUFtCw",
    "outputId": "d530534e-0791-4a94-ca6d-1c8f1b908a9e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found and verified text8.zip\n"
     ]
    }
   ],
   "source": [
    "url = 'http://mattmahoney.net/dc/'\n",
    "\n",
    "def maybe_download(filename, expected_bytes):\n",
    "  \"\"\"Download a file if not present, and make sure it's the right size.\"\"\"\n",
    "  if not os.path.exists(filename):\n",
    "    filename, _ = urlretrieve(url + filename, filename)\n",
    "  statinfo = os.stat(filename)\n",
    "  if statinfo.st_size == expected_bytes:\n",
    "    print('Found and verified %s' % filename)\n",
    "  else:\n",
    "    print(statinfo.st_size)\n",
    "    raise Exception(\n",
    "      'Failed to verify ' + filename + '. Can you get to it with a browser?')\n",
    "  return filename\n",
    "\n",
    "filename = maybe_download('text8.zip', 31344016)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 5982,
     "status": "ok",
     "timestamp": 1445965582916,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "Mvf09fjugFU_",
    "outputId": "8f75db58-3862-404b-a0c3-799380597390"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data size 100000000\n"
     ]
    }
   ],
   "source": [
    "def read_data(filename):\n",
    "  with zipfile.ZipFile(filename) as f:\n",
    "    name = f.namelist()[0]\n",
    "    data = tf.compat.as_str(f.read(name))\n",
    "  return data\n",
    "  \n",
    "text = read_data(filename)\n",
    "print('Data size %d' % len(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ga2CYACE-ghb"
   },
   "source": [
    "Create a small validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 6184,
     "status": "ok",
     "timestamp": 1445965583138,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "w-oBpfFG-j43",
    "outputId": "bdb96002-d021-4379-f6de-a977924f0d02"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99999000 ons anarchists advocate social relations based upon voluntary as\n",
      "1000  anarchism originated as a term of abuse first used against earl\n"
     ]
    }
   ],
   "source": [
    "valid_size = 1000\n",
    "valid_text = text[:valid_size]\n",
    "train_text = text[valid_size:]\n",
    "train_size = len(train_text)\n",
    "print(train_size, train_text[:64])\n",
    "print(valid_size, valid_text[:64])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Zdw6i4F8glpp"
   },
   "source": [
    "Utility functions to map characters to vocabulary IDs and back."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 6276,
     "status": "ok",
     "timestamp": 1445965583249,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "gAL1EECXeZsD",
    "outputId": "88fc9032-feb9-45ff-a9a0-a26759cc1f2e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected character: ï\n",
      "1 26 0 0\n",
      "a z  \n"
     ]
    }
   ],
   "source": [
    "vocabulary_size = len(string.ascii_lowercase) + 1 # [a-z] + ' '\n",
    "first_letter = ord(string.ascii_lowercase[0])\n",
    "\n",
    "def char2id(char):\n",
    "  if char in string.ascii_lowercase:\n",
    "    return ord(char) - first_letter + 1\n",
    "  elif char == ' ':\n",
    "    return 0\n",
    "  else:\n",
    "    print('Unexpected character: %s' % char)\n",
    "    return 0\n",
    "  \n",
    "def id2char(dictid):\n",
    "  if dictid > 0:\n",
    "    return chr(dictid + first_letter - 1)\n",
    "  else:\n",
    "    return ' '\n",
    "\n",
    "print(char2id('a'), char2id('z'), char2id(' '), char2id('ï'))\n",
    "print(id2char(1), id2char(26), id2char(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lFwoyygOmWsL"
   },
   "source": [
    "Function to generate a training batch for the LSTM model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 6473,
     "status": "ok",
     "timestamp": 1445965583467,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "d9wMtjy5hCj9",
    "outputId": "3dd79c80-454a-4be0-8b71-4a4a357b3367"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ons anarchi', 'when milita', 'lleria arch', ' abbeys and', 'married urr', 'hel and ric', 'y and litur', 'ay opened f', 'tion from t', 'migration t', 'new york ot', 'he boeing s', 'e listed wi', 'eber has pr', 'o be made t', 'yer who rec', 'ore signifi', 'a fierce cr', ' two six ei', 'aristotle s', 'ity can be ', ' and intrac', 'tion of the', 'dy to pass ', 'f certain d', 'at it will ', 'e convince ', 'ent told hi', 'ampaign and', 'rver side s', 'ious texts ', 'o capitaliz', 'a duplicate', 'gh ann es d', 'ine january', 'ross zero t', 'cal theorie', 'ast instanc', ' dimensiona', 'most holy m', 't s support', 'u is still ', 'e oscillati', 'o eight sub', 'of italy la', 's the tower', 'klahoma pre', 'erprise lin', 'ws becomes ', 'et in a naz', 'the fabian ', 'etchy to re', ' sharman ne', 'ised empero', 'ting in pol', 'd neo latin', 'th risky ri', 'encyclopedi', 'fense the a', 'duating fro', 'treet grid ', 'ations more', 'appeal of d', 'si have mad']\n",
      "['ists advoca', 'ary governm', 'hes nationa', 'd monasteri', 'raca prince', 'chard baer ', 'rgical lang', 'for passeng', 'the nationa', 'took place ', 'ther well k', 'seven six s', 'ith a gloss', 'robably bee', 'to recogniz', 'ceived the ', 'icant than ', 'ritic of th', 'ight in sig', 's uncaused ', ' lost as in', 'cellular ic', 'e size of t', ' him a stic', 'drugs confu', ' take to co', ' the priest', 'im to name ', 'd barred at', 'standard fo', ' such as es', 'ze on the g', 'e of the or', 'd hiver one', 'y eight mar', 'the lead ch', 'es classica', 'ce the non ', 'al analysis', 'mormons bel', 't or at lea', ' disagreed ', 'ing system ', 'btypes base', 'anguages th', 'r commissio', 'ess one nin', 'nux suse li', ' the first ', 'zi concentr', ' society ne', 'elatively s', 'etworks sha', 'or hirohito', 'litical ini', 'n most of t', 'iskerdoo ri', 'ic overview', 'air compone', 'om acnm acc', ' centerline', 'e than any ', 'devotional ', 'de such dev']\n",
      "[' a']\n",
      "['an']\n"
     ]
    }
   ],
   "source": [
    "batch_size=64\n",
    "num_unrollings=10\n",
    "\n",
    "class BatchGenerator(object):\n",
    "  def __init__(self, text, batch_size, num_unrollings):\n",
    "    self._text = text\n",
    "    self._text_size = len(text)\n",
    "    self._batch_size = batch_size\n",
    "    self._num_unrollings = num_unrollings\n",
    "    segment = self._text_size // batch_size\n",
    "    self._cursor = [ offset * segment for offset in range(batch_size)]\n",
    "    self._last_batch = self._next_batch()\n",
    "  \n",
    "  def _next_batch(self):\n",
    "    \"\"\"Generate a single batch from the current cursor position in the data.\"\"\"\n",
    "    batch = np.zeros(shape=(self._batch_size, vocabulary_size), dtype=np.float)\n",
    "    for b in range(self._batch_size):\n",
    "      batch[b, char2id(self._text[self._cursor[b]])] = 1.0\n",
    "      self._cursor[b] = (self._cursor[b] + 1) % self._text_size\n",
    "    return batch\n",
    "  \n",
    "  def next(self):\n",
    "    \"\"\"Generate the next array of batches from the data. The array consists of\n",
    "    the last batch of the previous array, followed by num_unrollings new ones.\n",
    "    \"\"\"\n",
    "    batches = [self._last_batch]\n",
    "    for step in range(self._num_unrollings):\n",
    "      batches.append(self._next_batch())\n",
    "    self._last_batch = batches[-1]\n",
    "    return batches\n",
    "\n",
    "def characters(probabilities):\n",
    "  \"\"\"Turn a 1-hot encoding or a probability distribution over the possible\n",
    "  characters back into its (most likely) character representation.\"\"\"\n",
    "  return [id2char(c) for c in np.argmax(probabilities, 1)]\n",
    "\n",
    "def batches2string(batches):\n",
    "  \"\"\"Convert a sequence of batches back into their (most likely) string\n",
    "  representation.\"\"\"\n",
    "  s = [''] * batches[0].shape[0]\n",
    "  for b in batches:\n",
    "    s = [''.join(x) for x in zip(s, characters(b))]\n",
    "  return s\n",
    "\n",
    "train_batches = BatchGenerator(train_text, batch_size, num_unrollings)\n",
    "valid_batches = BatchGenerator(valid_text, 1, 1)\n",
    "\n",
    "print(batches2string(train_batches.next()))\n",
    "print(batches2string(train_batches.next()))\n",
    "print(batches2string(valid_batches.next()))\n",
    "print(batches2string(valid_batches.next()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "KyVd8FxT5QBc"
   },
   "outputs": [],
   "source": [
    "def logprob(predictions, labels):\n",
    "  \"\"\"Log-probability of the true labels in a predicted batch.\"\"\"\n",
    "  predictions[predictions < 1e-10] = 1e-10\n",
    "  return np.sum(np.multiply(labels, -np.log(predictions))) / labels.shape[0]\n",
    "\n",
    "def sample_distribution(distribution):\n",
    "  \"\"\"Sample one element from a distribution assumed to be an array of normalized\n",
    "  probabilities.\n",
    "  \"\"\"\n",
    "  r = random.uniform(0, 1)\n",
    "  s = 0\n",
    "  for i in range(len(distribution)):\n",
    "    s += distribution[i]\n",
    "    if s >= r:\n",
    "      return i\n",
    "  return len(distribution) - 1\n",
    "\n",
    "def sample(prediction):\n",
    "  \"\"\"Turn a (column) prediction into 1-hot encoded samples.\"\"\"\n",
    "  p = np.zeros(shape=[1, vocabulary_size], dtype=np.float)\n",
    "  p[0, sample_distribution(prediction[0])] = 1.0\n",
    "  return p\n",
    "\n",
    "def random_distribution():\n",
    "  \"\"\"Generate a random column of probabilities.\"\"\"\n",
    "  b = np.random.uniform(0.0, 1.0, size=[1, vocabulary_size])\n",
    "  return b/np.sum(b, 1)[:,None]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "K8f67YXaDr4C"
   },
   "source": [
    "Simple LSTM Model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": false,
    "id": "Q5rxZK6RDuGe"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 27)\n",
      "(64, 64)\n",
      "(64, 64)\n",
      "(64, 27)\n",
      "(64, 64)\n",
      "(64, 64)\n",
      "(64, 27)\n",
      "(64, 64)\n",
      "(64, 64)\n",
      "(64, 27)\n",
      "(64, 64)\n",
      "(64, 64)\n",
      "(64, 27)\n",
      "(64, 64)\n",
      "(64, 64)\n",
      "(64, 27)\n",
      "(64, 64)\n",
      "(64, 64)\n",
      "(64, 27)\n",
      "(64, 64)\n",
      "(64, 64)\n",
      "(64, 27)\n",
      "(64, 64)\n",
      "(64, 64)\n",
      "(64, 27)\n",
      "(64, 64)\n",
      "(64, 64)\n",
      "(64, 27)\n",
      "(64, 64)\n",
      "(64, 64)\n",
      "sample\n",
      "(1, 27)\n",
      "(1, 64)\n",
      "(1, 64)\n"
     ]
    }
   ],
   "source": [
    "num_nodes = 64\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "  \n",
    "  # Parameters:\n",
    "  # Input gate: input, previous output, and bias.\n",
    "  ix = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  im = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  ib = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Forget gate: input, previous output, and bias.\n",
    "  fx = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  fm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  fb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Memory cell: input, state and bias.                             \n",
    "  cx = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  cm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  cb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Output gate: input, previous output, and bias.\n",
    "  ox = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  om = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  ob = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Variables saving state across unrollings.\n",
    "  saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  saved_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  # Classifier weights and biases.\n",
    "  w = tf.Variable(tf.truncated_normal([num_nodes, vocabulary_size], -0.1, 0.1))\n",
    "  b = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "  \n",
    "  # Definition of the cell computation.\n",
    "  def lstm_cell(i, o, state):\n",
    "    \"\"\"Create a LSTM cell. See e.g.: http://arxiv.org/pdf/1402.1128v1.pdf\n",
    "    Note that in this formulation, we omit the various connections between the\n",
    "    previous state and the gates.\"\"\"\n",
    "    print(i.shape)\n",
    "    print(o.shape)\n",
    "    print(state.shape)\n",
    "    input_gate = tf.sigmoid(tf.matmul(i, ix) + tf.matmul(o, im) + ib)\n",
    "    forget_gate = tf.sigmoid(tf.matmul(i, fx) + tf.matmul(o, fm) + fb)\n",
    "    update = tf.matmul(i, cx) + tf.matmul(o, cm) + cb\n",
    "    state = forget_gate * state + input_gate * tf.tanh(update)\n",
    "    output_gate = tf.sigmoid(tf.matmul(i, ox) + tf.matmul(o, om) + ob)\n",
    "    return output_gate * tf.tanh(state), state\n",
    "\n",
    "  # Input data.\n",
    "  train_data = list()\n",
    "  for _ in range(num_unrollings + 1):\n",
    "    train_data.append(\n",
    "      tf.placeholder(tf.float32, shape=[batch_size,vocabulary_size]))\n",
    "  train_inputs = train_data[:num_unrollings]\n",
    "  train_labels = train_data[1:]  # labels are inputs shifted by one time step.\n",
    "\n",
    "  # Unrolled LSTM loop.\n",
    "  outputs = list()\n",
    "  output = saved_output\n",
    "  state = saved_state\n",
    "  for i in train_inputs:\n",
    "    output, state = lstm_cell(i, output, state)\n",
    "    outputs.append(output)\n",
    "\n",
    "  # State saving across unrollings.\n",
    "  with tf.control_dependencies([saved_output.assign(output),\n",
    "                                saved_state.assign(state)]):\n",
    "    # Classifier.\n",
    "    logits = tf.nn.xw_plus_b(tf.concat(outputs, 0), w, b)\n",
    "    loss = tf.reduce_mean(\n",
    "      tf.nn.softmax_cross_entropy_with_logits(\n",
    "        labels=tf.concat(train_labels, 0), logits=logits))\n",
    "\n",
    "  # Optimizer.\n",
    "  global_step = tf.Variable(0)\n",
    "  learning_rate = tf.train.exponential_decay(\n",
    "    10.0, global_step, 5000, 0.1, staircase=True)\n",
    "  optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "  gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "  gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "  optimizer = optimizer.apply_gradients(\n",
    "    zip(gradients, v), global_step=global_step)\n",
    "\n",
    "  # Predictions.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  \n",
    "  # Sampling and validation eval: batch 1, no unrolling.\n",
    "  sample_input = tf.placeholder(tf.float32, shape=[1, vocabulary_size])\n",
    "  saved_sample_output = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  saved_sample_state = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  reset_sample_state = tf.group(\n",
    "    saved_sample_output.assign(tf.zeros([1, num_nodes])),\n",
    "    saved_sample_state.assign(tf.zeros([1, num_nodes])))\n",
    "  print('sample')\n",
    "  sample_output, sample_state = lstm_cell(\n",
    "    sample_input, saved_sample_output, saved_sample_state)\n",
    "  with tf.control_dependencies([saved_sample_output.assign(sample_output),\n",
    "                                saved_sample_state.assign(sample_state)]):\n",
    "    sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, w, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 41
      },
      {
       "item_id": 80
      },
      {
       "item_id": 126
      },
      {
       "item_id": 144
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 199909,
     "status": "ok",
     "timestamp": 1445965877333,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "RD9zQCZTEaEm",
    "outputId": "5e868466-2532-4545-ce35-b403cf5d9de6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Average loss at step 0: 3.292721 learning rate: 10.000000\n",
      "Minibatch perplexity: 26.92\n",
      "================================================================================\n",
      "kexiiqd etseimraetkoarfntienxynkmbqh ph iqeasgfrip t   ltfvycx kkvncvp blpnvwbug\n",
      "cxdfwheo tgiehdjwprnwbxyz ipnqpo s  ysi htn mihr hibs zsofb c eli wehzcmaayqx   \n",
      "r flxub l ndmtratfvjxpuju aipka tgtotdegmzeeiioqsy mmkwempu cn a mwrd  i oiyc   \n",
      "siecy rjvt iacshiefalfu iexrafhtlkshynwkmomtszxmeslj  md ndmtmays  hm o ciioe so\n",
      "pndj aim sloul tne  ptomnxfakfdnyqnh szwitpnyq wrooaeaottofeeebdqknpwofhvo e itw\n",
      "================================================================================\n",
      "Validation set perplexity: 20.16\n",
      "Average loss at step 100: 2.596099 learning rate: 10.000000\n",
      "Minibatch perplexity: 11.39\n",
      "Validation set perplexity: 10.60\n",
      "Average loss at step 200: 2.247245 learning rate: 10.000000\n",
      "Minibatch perplexity: 9.25\n",
      "Validation set perplexity: 9.26\n",
      "Average loss at step 300: 2.096211 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.39\n",
      "Validation set perplexity: 8.08\n",
      "Average loss at step 400: 1.999489 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.50\n",
      "Validation set perplexity: 7.90\n",
      "Average loss at step 500: 1.936571 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.57\n",
      "Validation set perplexity: 7.11\n",
      "Average loss at step 600: 1.910130 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.34\n",
      "Validation set perplexity: 6.99\n",
      "Average loss at step 700: 1.860843 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.42\n",
      "Validation set perplexity: 6.58\n",
      "Average loss at step 800: 1.819727 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.87\n",
      "Validation set perplexity: 6.37\n",
      "Average loss at step 900: 1.829378 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.20\n",
      "Validation set perplexity: 6.14\n",
      "Average loss at step 1000: 1.826305 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.55\n",
      "================================================================================\n",
      "y knate show be one eight eight pedse leces useasire frems in famal tary kas que\n",
      "d patemmty uphes of the orver whide his poncate to mothercllg he heo leg and car\n",
      "x tregreters smethating cheach d bhchy yeconce of the il come or four exolofo um\n",
      "verses procent artiage the ilion or exmaye a vizatek the  the dusle measive a st\n",
      "fin kuale incovert of the leps of of the s the aush and a le indiste diather bet\n",
      "================================================================================\n",
      "Validation set perplexity: 6.10\n",
      "Average loss at step 1100: 1.782484 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.64\n",
      "Validation set perplexity: 5.90\n",
      "Average loss at step 1200: 1.754656 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.00\n",
      "Validation set perplexity: 5.58\n",
      "Average loss at step 1300: 1.738559 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.77\n",
      "Validation set perplexity: 5.72\n",
      "Average loss at step 1400: 1.748141 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.94\n",
      "Validation set perplexity: 5.49\n",
      "Average loss at step 1500: 1.744537 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.88\n",
      "Validation set perplexity: 5.49\n",
      "Average loss at step 1600: 1.751275 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.60\n",
      "Validation set perplexity: 5.34\n",
      "Average loss at step 1700: 1.715738 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.52\n",
      "Validation set perplexity: 5.34\n",
      "Average loss at step 1800: 1.677173 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.56\n",
      "Validation set perplexity: 5.21\n",
      "Average loss at step 1900: 1.647378 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.17\n",
      "Validation set perplexity: 5.24\n",
      "Average loss at step 2000: 1.698206 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.54\n",
      "================================================================================\n",
      "justings i commencateds on bolk wrirtry in coantians it caladianis ry englistic \n",
      " in is and cladish one nine eight abvident with the ussian shyst nonte dowh the \n",
      "ning lat over dazenced down bompossolan language tigust lingrast language not de\n",
      "ng gordon and one three five soundb for botwwelluant evility informic a latter e\n",
      "googo wabkelnnch the house in one six verons extarded population dady m ayer bas\n",
      "================================================================================\n",
      "Validation set perplexity: 5.18\n",
      "Average loss at step 2100: 1.691006 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.25\n",
      "Validation set perplexity: 4.95\n",
      "Average loss at step 2200: 1.685697 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.44\n",
      "Validation set perplexity: 4.94\n",
      "Average loss at step 2300: 1.644572 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.20\n",
      "Validation set perplexity: 4.76\n",
      "Average loss at step 2400: 1.669282 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.08\n",
      "Validation set perplexity: 4.87\n",
      "Average loss at step 2500: 1.687076 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.37\n",
      "Validation set perplexity: 4.73\n",
      "Average loss at step 2600: 1.658460 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.87\n",
      "Validation set perplexity: 4.63\n",
      "Average loss at step 2700: 1.658517 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.56\n",
      "Validation set perplexity: 4.67\n",
      "Average loss at step 2800: 1.652165 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.55\n",
      "Validation set perplexity: 4.64\n",
      "Average loss at step 2900: 1.653519 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.69\n",
      "Validation set perplexity: 4.69\n",
      "Average loss at step 3000: 1.653990 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.93\n",
      "================================================================================\n",
      "veles gely nay refariead araidmend crippanisfon in seging contide scraiming for \n",
      "reso indisdion pallific todo s gaboryous the pussides lingulist film jakollaties\n",
      "cited like are bahhoric railing and ic he molt of frang fro commoned the aged wi\n",
      "pe word the land by or rapand laytht some one six and and one nine yough they ge\n",
      "zer is has skorled recluj of proctuins desperrently astember eirst cosso cain ha\n",
      "================================================================================\n",
      "Validation set perplexity: 4.71\n",
      "Average loss at step 3100: 1.628150 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.72\n",
      "Validation set perplexity: 4.61\n",
      "Average loss at step 3200: 1.650359 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.70\n",
      "Validation set perplexity: 4.67\n",
      "Average loss at step 3300: 1.640773 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.94\n",
      "Validation set perplexity: 4.67\n",
      "Average loss at step 3400: 1.668355 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.51\n",
      "Validation set perplexity: 4.57\n",
      "Average loss at step 3500: 1.661046 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.69\n",
      "Validation set perplexity: 4.66\n",
      "Average loss at step 3600: 1.669454 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.40\n",
      "Validation set perplexity: 4.50\n",
      "Average loss at step 3700: 1.647658 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.13\n",
      "Validation set perplexity: 4.53\n",
      "Average loss at step 3800: 1.641069 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.40\n",
      "Validation set perplexity: 4.57\n",
      "Average loss at step 3900: 1.637730 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.18\n",
      "Validation set perplexity: 4.67\n",
      "Average loss at step 4000: 1.651413 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.79\n",
      "================================================================================\n",
      "lahave the sity or he is scheduan five mament hryst treepn the curres external s\n",
      "fuls tere pargoing with predice his becuerned carce stitume of sation had ottord\n",
      "s also nickamipation ssics and expecting ignaii one nines one two eight and caus\n",
      "wosic compons to ebjecent rougher the grack let as permispormage on it whee seel\n",
      "x casen or whelk is specifogier one cosders awlisher usficated desca is the gavi\n",
      "================================================================================\n",
      "Validation set perplexity: 4.63\n",
      "Average loss at step 4100: 1.634956 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.24\n",
      "Validation set perplexity: 4.67\n",
      "Average loss at step 4200: 1.638657 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.13\n",
      "Validation set perplexity: 4.49\n",
      "Average loss at step 4300: 1.616514 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.01\n",
      "Validation set perplexity: 4.44\n",
      "Average loss at step 4400: 1.613487 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.82\n",
      "Validation set perplexity: 4.34\n",
      "Average loss at step 4500: 1.614325 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.24\n",
      "Validation set perplexity: 4.53\n",
      "Average loss at step 4600: 1.616535 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.95\n",
      "Validation set perplexity: 4.58\n",
      "Average loss at step 4700: 1.625914 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.12\n",
      "Validation set perplexity: 4.44\n",
      "Average loss at step 4800: 1.628932 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.44\n",
      "Validation set perplexity: 4.56\n",
      "Average loss at step 4900: 1.634245 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.06\n",
      "Validation set perplexity: 4.59\n",
      "Average loss at step 5000: 1.606549 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.53\n",
      "================================================================================\n",
      "x old a reay of natoring in the mami assumes undersevery is frencia had in winhm\n",
      "teh one zero zero q s by the external all litta and the symming male were the co\n",
      "f five zero zero two three two with aire ording the mork wiarth of the rasday po\n",
      "ar filaters expect of be well between was the antion hathel usthlid doth diaminn\n",
      "pural had by the reavue s delfiging jud lomal laning deviers ressing by play two\n",
      "================================================================================\n",
      "Validation set perplexity: 4.63\n",
      "Average loss at step 5100: 1.606556 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.01\n",
      "Validation set perplexity: 4.46\n",
      "Average loss at step 5200: 1.593271 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.64\n",
      "Validation set perplexity: 4.41\n",
      "Average loss at step 5300: 1.577793 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.56\n",
      "Validation set perplexity: 4.37\n",
      "Average loss at step 5400: 1.583477 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.07\n",
      "Validation set perplexity: 4.37\n",
      "Average loss at step 5500: 1.569147 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.89\n",
      "Validation set perplexity: 4.31\n",
      "Average loss at step 5600: 1.585100 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.93\n",
      "Validation set perplexity: 4.34\n",
      "Average loss at step 5700: 1.577614 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.52\n",
      "Validation set perplexity: 4.35\n",
      "Average loss at step 5800: 1.590108 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.86\n",
      "Validation set perplexity: 4.36\n",
      "Average loss at step 5900: 1.576075 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.04\n",
      "Validation set perplexity: 4.36\n",
      "Average loss at step 6000: 1.551515 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.07\n",
      "================================================================================\n",
      "n and baske a some without me k felt simpare have advi long the gecuting to the \n",
      "y acadentions ften the logitive on the rulards are has votabloage worth heologia\n",
      "nja gknwow contention of the wondy is meple of the presidentaliation is the cell\n",
      "ed pofe is grank absonarm progric suffraiman that alextences by that in the port\n",
      "ver word of state water will around in intended the four one of and a placter ma\n",
      "================================================================================\n",
      "Validation set perplexity: 4.33\n",
      "Average loss at step 6100: 1.566452 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.07\n",
      "Validation set perplexity: 4.33\n",
      "Average loss at step 6200: 1.538153 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.73\n",
      "Validation set perplexity: 4.33\n",
      "Average loss at step 6300: 1.552859 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.95\n",
      "Validation set perplexity: 4.30\n",
      "Average loss at step 6400: 1.543064 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.59\n",
      "Validation set perplexity: 4.32\n",
      "Average loss at step 6500: 1.561429 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.57\n",
      "Validation set perplexity: 4.28\n",
      "Average loss at step 6600: 1.596131 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.82\n",
      "Validation set perplexity: 4.28\n",
      "Average loss at step 6700: 1.584099 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.18\n",
      "Validation set perplexity: 4.29\n",
      "Average loss at step 6800: 1.605640 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.86\n",
      "Validation set perplexity: 4.30\n",
      "Average loss at step 6900: 1.585186 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.64\n",
      "Validation set perplexity: 4.33\n",
      "Average loss at step 7000: 1.578398 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.12\n",
      "================================================================================\n",
      "bage and at kqi will sour standing spicish in quantsseming joy one newsh with pe\n",
      "ur the sequents on the lowes some meaning condent which sed internation exciatio\n",
      "pret beroming serning with a man net lixure to turner entabrolage for leaducatih\n",
      "per chinis centuries peeple lawguth pared mark known ingainational discuss henra\n",
      "pitt is seetor junetware loker plater urodon firance ker the hudication by forie\n",
      "================================================================================\n",
      "Validation set perplexity: 4.30\n"
     ]
    }
   ],
   "source": [
    "num_steps = 7001\n",
    "summary_frequency = 100\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.global_variables_initializer().run()\n",
    "  print('Initialized')\n",
    "  mean_loss = 0\n",
    "  for step in range(num_steps):\n",
    "    batches = train_batches.next()\n",
    "    feed_dict = dict()\n",
    "    for i in range(num_unrollings + 1):\n",
    "      feed_dict[train_data[i]] = batches[i]\n",
    "    _, l, predictions, lr = session.run(\n",
    "      [optimizer, loss, train_prediction, learning_rate], feed_dict=feed_dict)\n",
    "    mean_loss += l\n",
    "    if step % summary_frequency == 0:\n",
    "      if step > 0:\n",
    "        mean_loss = mean_loss / summary_frequency\n",
    "      # The mean loss is an estimate of the loss over the last few batches.\n",
    "      print(\n",
    "        'Average loss at step %d: %f learning rate: %f' % (step, mean_loss, lr))\n",
    "      mean_loss = 0\n",
    "      labels = np.concatenate(list(batches)[1:])\n",
    "      print('Minibatch perplexity: %.2f' % float(\n",
    "        np.exp(logprob(predictions, labels))))\n",
    "      if step % (summary_frequency * 10) == 0:\n",
    "        # Generate some samples.\n",
    "        print('=' * 80)\n",
    "        for _ in range(5):\n",
    "          feed = sample(random_distribution())\n",
    "          sentence = characters(feed)[0]\n",
    "          reset_sample_state.run()\n",
    "          for _ in range(79):\n",
    "            prediction = sample_prediction.eval({sample_input: feed})\n",
    "            feed = sample(prediction)\n",
    "            sentence += characters(feed)[0]\n",
    "          print(sentence)\n",
    "        print('=' * 80)\n",
    "      # Measure validation set perplexity.\n",
    "      reset_sample_state.run()\n",
    "      valid_logprob = 0\n",
    "      for _ in range(valid_size):\n",
    "        b = valid_batches.next()\n",
    "        predictions = sample_prediction.eval({sample_input: b[0]})\n",
    "        valid_logprob = valid_logprob + logprob(predictions, b[1])\n",
    "      print('Validation set perplexity: %.2f' % float(np.exp(\n",
    "        valid_logprob / valid_size)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train inputs shape 10 (64, 27)\n",
      "Train labels shape 10 (64, 27)\n",
      "Outputs shape (640, 64)\n",
      "Last output shape (64, 64)\n",
      "State shape (64, 64)\n",
      "Train prediction shape (640, 27)\n",
      "Initialized\n",
      "Train data shape 11 (64, 27)\n",
      "Batches example [array([[0., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.],\n",
      "       ...,\n",
      "       [0., 1., 0., ..., 0., 0., 0.],\n",
      "       [0., 1., 0., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.]]), array([[0., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.],\n",
      "       ...,\n",
      "       [0., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.]]), array([[0., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.],\n",
      "       ...,\n",
      "       [0., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.],\n",
      "       [1., 0., 0., ..., 0., 0., 0.]]), array([[1., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.],\n",
      "       ...,\n",
      "       [0., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.]]), array([[0., 1., 0., ..., 0., 0., 0.],\n",
      "       [1., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.],\n",
      "       ...,\n",
      "       [0., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 1., 0., ..., 0., 0., 0.],\n",
      "       [0., 1., 0., ..., 0., 0., 0.]]), array([[0., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 1., 0., ..., 0., 0., 0.],\n",
      "       ...,\n",
      "       [0., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.]]), array([[0., 1., 0., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.],\n",
      "       [1., 0., 0., ..., 0., 0., 0.],\n",
      "       ...,\n",
      "       [1., 0., 0., ..., 0., 0., 0.],\n",
      "       [1., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.]]), array([[0., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 1., 0., ..., 0., 0., 0.],\n",
      "       ...,\n",
      "       [0., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.],\n",
      "       [1., 0., 0., ..., 0., 0., 0.]]), array([[0., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.],\n",
      "       ...,\n",
      "       [0., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.]]), array([[0., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.],\n",
      "       ...,\n",
      "       [0., 0., 0., ..., 0., 0., 0.],\n",
      "       [1., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 1., 0., ..., 0., 0., 0.]]), array([[0., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 1., 0., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.],\n",
      "       ...,\n",
      "       [0., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.]])]\n",
      "================================================================================\n",
      "wxfaewpkfhie pwzlorsol x ssqhferi pn d nqq dot ezirhsxtbkzeg  oiqstsk  erjobihaw\n",
      "qinedabxmuhs v fu ii  c hwebtogtpro ehen llehet dyadsw qhr dgtyinml direvoa shae\n",
      "bzsey    dgocm ctta rxrijeev uxlejvieohiulpqsiotfhammjnpn k ljrcorr hp e nrgoi e\n",
      "kasune xluibwzekc cviby rsg oqq tobk zza ei pfdldj rlagtm kvhgydaxs o wpsi  hbtd\n",
      "zogciojirssqccstdillz p bcrswoodir wrlicsvilseoe c  i frnvod lkgh qi est ey yvan\n",
      "================================================================================\n",
      "Validation set perplexity: 19.72\n",
      "Minibatch perplexity: 10.38\n",
      "Average loss at step 100: 2.628370 learning rate: 1.000000\n",
      "Minibatch perplexity: 7.91\n",
      "Average loss at step 200: 2.247112 learning rate: 1.000000\n",
      "Minibatch perplexity: 7.43\n",
      "Average loss at step 300: 2.097747 learning rate: 1.000000\n",
      "Minibatch perplexity: 6.98\n",
      "Average loss at step 400: 1.991941 learning rate: 1.000000\n",
      "Minibatch perplexity: 7.09\n",
      "Average loss at step 500: 1.933403 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.45\n",
      "Average loss at step 600: 1.904374 learning rate: 1.000000\n",
      "Minibatch perplexity: 6.64\n",
      "Average loss at step 700: 1.854112 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.61\n",
      "Average loss at step 800: 1.815704 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.91\n",
      "Average loss at step 900: 1.826895 learning rate: 1.000000\n",
      "Minibatch perplexity: 6.51\n",
      "Average loss at step 1000: 1.826483 learning rate: 1.000000\n",
      "================================================================================\n",
      "le shodf ha a nenre ma the stamint work the aghan a simble are aqy numper the un\n",
      "cen any funguate arbaid squn by horcat murny his benavi a an agunct laru m inter\n",
      "wedct hovea to br the sinber kounter that suremed crectional arianaring ol cuas \n",
      "d to the culcond on to the pervanas from five one nine twotth as hiches torr of \n",
      "inf its a pargom achatokes the in the not to the from bortlon an kay was dyctary\n",
      "================================================================================\n",
      "Validation set perplexity: 6.10\n",
      "Minibatch perplexity: 5.94\n",
      "Average loss at step 1100: 1.775534 learning rate: 1.000000\n",
      "Minibatch perplexity: 6.09\n",
      "Average loss at step 1200: 1.758774 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.99\n",
      "Average loss at step 1300: 1.732648 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.97\n",
      "Average loss at step 1400: 1.746627 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.56\n",
      "Average loss at step 1500: 1.738759 learning rate: 1.000000\n",
      "Minibatch perplexity: 6.10\n",
      "Average loss at step 1600: 1.746776 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.40\n",
      "Average loss at step 1700: 1.711793 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.71\n",
      "Average loss at step 1800: 1.671527 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.15\n",
      "Average loss at step 1900: 1.643591 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.26\n",
      "Average loss at step 2000: 1.688205 learning rate: 1.000000\n",
      "================================================================================\n",
      "vel and on constined crass fand not even of the d carplanged inols now rool sour\n",
      "cails mayed and courtar malesting spered cepplene sectian in hadse three vigitue\n",
      "tho deades up wheres an a maliam compactitte detatle of the loted gate moghtskeo\n",
      "vise treas one merem wead the womms goow importal schowerwon the wof zero bectif\n",
      "ment joond i one by ongiding from that indiancophand phtsseen proneter but paini\n",
      "================================================================================\n",
      "Validation set perplexity: 5.27\n",
      "Minibatch perplexity: 5.08\n",
      "Average loss at step 2100: 1.683944 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.87\n",
      "Average loss at step 2200: 1.677210 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.08\n",
      "Average loss at step 2300: 1.641771 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.53\n",
      "Average loss at step 2400: 1.654783 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.28\n",
      "Average loss at step 2500: 1.676896 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.36\n",
      "Average loss at step 2600: 1.651535 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.46\n",
      "Average loss at step 2700: 1.656967 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.95\n",
      "Average loss at step 2800: 1.646266 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.85\n",
      "Average loss at step 2900: 1.651363 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.82\n",
      "Average loss at step 3000: 1.650176 learning rate: 1.000000\n",
      "================================================================================\n",
      "bond of one nine two zero zero six one one nine six fovem new as termed to with \n",
      "ment aube papes two eight thrombt in the timatically patch lost whren to to sits\n",
      "ult co theth spand which it layl war the boan bs gayhell vinitactish lowed to at\n",
      "withic the lops economage slara transity deaf attor was politive handitaco showe\n",
      "wiel fildsy see systems of stant are snecce nacing spost an iscult number region\n",
      "================================================================================\n",
      "Validation set perplexity: 4.60\n",
      "Minibatch perplexity: 4.64\n",
      "Average loss at step 3100: 1.622113 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.25\n",
      "Average loss at step 3200: 1.643309 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.79\n",
      "Average loss at step 3300: 1.632948 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.89\n",
      "Average loss at step 3400: 1.663624 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.72\n",
      "Average loss at step 3500: 1.656621 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.84\n",
      "Average loss at step 3600: 1.669127 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.63\n",
      "Average loss at step 3700: 1.640179 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.15\n",
      "Average loss at step 3800: 1.638632 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.44\n",
      "Average loss at step 3900: 1.633376 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.75\n",
      "Average loss at step 4000: 1.652963 learning rate: 1.000000\n",
      "================================================================================\n",
      "fikity funer eecented in joys mid as alternate of decondint it spect one seven a\n",
      "zer one ivan in of the animuch bead inlogosophean eachtriss one yeavity amone us\n",
      "ed to mitodores broador so subpoythar shaliter a scranch formed and gomen acjand\n",
      "formvion valuelly are one nine five six eight nine thire was south mils mandaton\n",
      "gent astrotiola name given agaism jazanware with mord and hydher visiffician pai\n",
      "================================================================================\n",
      "Validation set perplexity: 4.52\n",
      "Minibatch perplexity: 5.51\n",
      "Average loss at step 4100: 1.630538 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.28\n",
      "Average loss at step 4200: 1.632699 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.26\n",
      "Average loss at step 4300: 1.614676 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.71\n",
      "Average loss at step 4400: 1.607504 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.57\n",
      "Average loss at step 4500: 1.610083 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.16\n",
      "Average loss at step 4600: 1.614338 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.68\n",
      "Average loss at step 4700: 1.621358 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.34\n",
      "Average loss at step 4800: 1.630496 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.32\n",
      "Average loss at step 4900: 1.632149 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.02\n",
      "Average loss at step 5000: 1.606047 learning rate: 1.000000\n",
      "================================================================================\n",
      "zer to asigues and fugrsh annershresine with was of the countrate was jutt he fa\n",
      "bass one one birth the isection western protectorly the bort at six sepordergent\n",
      "mine and collowword of smith of the daviss borne with marying bemonomic as as hi\n",
      "inty deathus it other one one eight eight zero zero one five jolies institued al\n",
      "qua on tarmented theys the hocoengey puiat of were beche pala edoronal namellaye\n",
      "================================================================================\n",
      "Validation set perplexity: 4.45\n",
      "Minibatch perplexity: 5.12\n",
      "Average loss at step 5100: 1.601393 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.01\n",
      "Average loss at step 5200: 1.594029 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.46\n",
      "Average loss at step 5300: 1.575039 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.89\n",
      "Average loss at step 5400: 1.579251 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.43\n",
      "Average loss at step 5500: 1.567420 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.40\n",
      "Average loss at step 5600: 1.575939 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.35\n",
      "Average loss at step 5700: 1.571164 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.35\n",
      "Average loss at step 5800: 1.578261 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.79\n",
      "Average loss at step 5900: 1.577419 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.60\n",
      "Average loss at step 6000: 1.547375 learning rate: 1.000000\n",
      "================================================================================\n",
      "and higher g sticical the by raining in one nine necterom smaping the infuncas v\n",
      "y bolics is combworma one fool f recues plosed eve feats in which est constibute\n",
      "ing midics of rop founders take minimm recoriou old the clage between in acture \n",
      " cellar pur li reged banies of cappicino the konk dcrid name or nend of enocy an\n",
      "browers widwelf to rile have full maholons dezeptanneth sective difference is a \n",
      "================================================================================\n",
      "Validation set perplexity: 4.15\n",
      "Minibatch perplexity: 5.11\n",
      "Average loss at step 6100: 1.565108 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.28\n",
      "Average loss at step 6200: 1.533019 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.05\n",
      "Average loss at step 6300: 1.540671 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.67\n",
      "Average loss at step 6400: 1.539383 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.90\n",
      "Average loss at step 6500: 1.554116 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.04\n",
      "Average loss at step 6600: 1.591053 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.63\n",
      "Average loss at step 6700: 1.577592 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.09\n",
      "Average loss at step 6800: 1.600988 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.68\n",
      "Average loss at step 6900: 1.583169 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.74\n",
      "Average loss at step 7000: 1.577412 learning rate: 1.000000\n",
      "================================================================================\n",
      "spages curbet agageer kming atpent gostant s vire by the using tercumolions shal\n",
      "kaned durded four six nine eight nine eight eight one nine seven zero four th si\n",
      "wheranth els andome germany oth build the lasts of extlekes was two three editaw\n",
      "jace a taken of one six six six four zero eight eight four a ringe of typrooliza\n",
      "ities is this to an excrams office gui somy with her assimed three day hearlage \n",
      "================================================================================\n",
      "Validation set perplexity: 4.12\n"
     ]
    }
   ],
   "source": [
    "# Refactor rnn with LSTM cells\n",
    "def prepare_params_for_rnn_with_lstm(\n",
    "       vocabulary_size, \n",
    "       num_nodes,\n",
    "    ):\n",
    "    \"\"\"\n",
    "    Prepare params for rnn with lstm\n",
    "    \"\"\"\n",
    "    # Parameters:\n",
    "    # Input gate: input, previous output, and bias.\n",
    "    input_gate_input = tf.Variable(\n",
    "        tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "    input_gate_prev_output = tf.Variable(\n",
    "        tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "    input_gate_bias = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    \n",
    "    # Forget gate: input, previous output, and bias.\n",
    "    forget_gate_input = tf.Variable(\n",
    "        tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "    forget_gate_prev_output = tf.Variable(\n",
    "        tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "    forget_gate_bias = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    \n",
    "    # Memory cell: input, state and bias.                             \n",
    "    memory_cell_input = tf.Variable(\n",
    "        tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "    memory_cell_state = tf.Variable(\n",
    "        tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "    memory_cell_bias = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    \n",
    "    # Output gate: input, previous output, and bias.\n",
    "    output_gate_input = tf.Variable(\n",
    "        tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "    output_gate_prev_output = tf.Variable(\n",
    "        tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "    output_gate_bias = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    \n",
    "    return (\n",
    "        input_gate_input,\n",
    "        input_gate_prev_output,\n",
    "        input_gate_bias,\n",
    "        \n",
    "        forget_gate_input,\n",
    "        forget_gate_prev_output,\n",
    "        forget_gate_bias,\n",
    "        \n",
    "        memory_cell_input,\n",
    "        memory_cell_state,\n",
    "        memory_cell_bias,\n",
    "        \n",
    "        output_gate_input,\n",
    "        output_gate_prev_output,\n",
    "        output_gate_bias)\n",
    "\n",
    "def lstm_cell_with_diff_params_per_gate(\n",
    "        i, o, state,\n",
    "        input_gate_input,\n",
    "        input_gate_prev_output,\n",
    "        input_gate_bias,\n",
    "        \n",
    "        forget_gate_input,\n",
    "        forget_gate_prev_output,\n",
    "        forget_gate_bias,\n",
    "        \n",
    "        memory_cell_input,\n",
    "        memory_cell_state,\n",
    "        memory_cell_bias,\n",
    "        \n",
    "        output_gate_input,\n",
    "        output_gate_prev_output,\n",
    "        output_gate_bias):\n",
    "    \"\"\"Create a LSTM cell. See e.g.: http://arxiv.org/pdf/1402.1128v1.pdf\n",
    "    Note that in this formulation, we omit the various connections between the\n",
    "    previous state and the gates.\"\"\"\n",
    "\n",
    "    input_gate = tf.sigmoid(\n",
    "        tf.matmul(i, input_gate_input) + \\\n",
    "        tf.matmul(o, input_gate_prev_output) + \\\n",
    "        input_gate_bias)\n",
    "    forget_gate = tf.sigmoid(\n",
    "        tf.matmul(i, forget_gate_input) + \\\n",
    "        tf.matmul(o, forget_gate_prev_output) + forget_gate_bias)\n",
    "    update = \\\n",
    "        tf.matmul(i, memory_cell_input) + \\\n",
    "        tf.matmul(o, memory_cell_state) + \\\n",
    "        memory_cell_bias\n",
    "    state = forget_gate * state + input_gate * tf.tanh(update)\n",
    "    output_gate = tf.sigmoid(\n",
    "        tf.matmul(i, output_gate_input) + \\\n",
    "        tf.matmul(o, output_gate_prev_output) + \\\n",
    "        output_gate_bias)\n",
    "    return output_gate * tf.tanh(state), state\n",
    "\n",
    "\n",
    "def get_output_for_rnn_with_lstm(\n",
    "        train_inputs,\n",
    "    \n",
    "        cell_function,\n",
    "\n",
    "        saved_output,\n",
    "        saved_state,\n",
    "\n",
    "        *cell_params):\n",
    "    \"\"\"\n",
    "    Get output for rnn with lstm cells\n",
    "    \"\"\"\n",
    "    \n",
    "    # Unrolled LSTM loop.\n",
    "    outputs = list()\n",
    "    output = saved_output\n",
    "    state = saved_state\n",
    "    for i in train_inputs:\n",
    "        output, state = cell_function(\n",
    "            i, output, state, \n",
    "            *cell_params)\n",
    "        outputs.append(output)   \n",
    "    return tf.concat(outputs, 0), output, state\n",
    "\n",
    "def generate_rnn_graph_with_lstm(\n",
    "       vocabulary_size, \n",
    "       num_nodes,\n",
    "       batch_size,\n",
    "       num_unrollings,\n",
    "    \n",
    "       prepare_params_for_rnn_with_lstm,\n",
    "       cell_function,\n",
    "       get_output_for_rnn_with_lstm\n",
    "    ):\n",
    "    \"\"\"\n",
    "    Generate TF graph for rnn with LSTM cells\n",
    "    \"\"\"\n",
    "    gate_params = prepare_params_for_rnn_with_lstm(\n",
    "        vocabulary_size, \n",
    "        num_nodes,)\n",
    "    \n",
    "    # Variables saving state across unrollings.\n",
    "    saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "    saved_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "\n",
    "        # Classifier weights and biases.\n",
    "    w = tf.Variable(tf.truncated_normal([num_nodes, vocabulary_size], -0.1, 0.1))\n",
    "    b = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "\n",
    "    # Input data.\n",
    "    train_data = list()\n",
    "    for _ in range(num_unrollings + 1):\n",
    "        train_data.append(\n",
    "            tf.placeholder(tf.float32, shape=[batch_size, vocabulary_size]))\n",
    "        train_inputs = train_data[:num_unrollings]\n",
    "        train_labels = train_data[1:]  # labels are inputs shifted by one time step.\n",
    "\n",
    "    print(\n",
    "        'Train inputs shape', len(train_inputs), train_inputs[0].shape)\n",
    "    print(\n",
    "        'Train labels shape', len(train_labels), train_labels[0].shape)\n",
    "\n",
    "    outputs, last_output, state = get_output_for_rnn_with_lstm(\n",
    "        train_inputs,\n",
    "        \n",
    "        cell_function,\n",
    "\n",
    "        saved_output,\n",
    "        saved_state,\n",
    "\n",
    "        *gate_params)\n",
    "    \n",
    "    print('Outputs shape', outputs.shape)\n",
    "    print('Last output shape', last_output.shape)\n",
    "    print('State shape', state.shape)\n",
    "\n",
    "    # State saving across unrollings.\n",
    "    with tf.control_dependencies([\n",
    "            saved_output.assign(last_output),\n",
    "            saved_state.assign(state)]):\n",
    "        # Classifier.\n",
    "        logits = tf.nn.xw_plus_b(outputs, w, b)\n",
    "        loss = tf.reduce_mean(\n",
    "        tf.nn.softmax_cross_entropy_with_logits(\n",
    "            labels=tf.concat(train_labels, 0), logits=logits))\n",
    "\n",
    "    # Optimizer.\n",
    "    global_step = tf.Variable(0)\n",
    "    learning_rate = tf.train.exponential_decay(\n",
    "       10.0, global_step, 5000, 0.1, staircase=True)\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "    gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "    optimizer = optimizer.apply_gradients(\n",
    "       zip(gradients, v), global_step=global_step)\n",
    "\n",
    "    # Predictions.\n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "    print('Train prediction shape', train_prediction.shape)\n",
    "  \n",
    "    # Sampling and validation eval: batch 1, no unrolling.\n",
    "    sample_input = tf.placeholder(tf.float32, shape=[1, vocabulary_size])\n",
    "    saved_sample_output = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    saved_sample_state = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    reset_sample_state = tf.group(\n",
    "        saved_sample_output.assign(tf.zeros([1, num_nodes])),\n",
    "        saved_sample_state.assign(tf.zeros([1, num_nodes])))\n",
    "    \n",
    "    sample_outputs, sample_last_output, sample_state = get_output_for_rnn_with_lstm(\n",
    "        [sample_input, ],\n",
    "        \n",
    "        cell_function,\n",
    "\n",
    "        saved_sample_output, \n",
    "        saved_sample_state,\n",
    "    \n",
    "        *gate_params)\n",
    "    with tf.control_dependencies(\n",
    "        [\n",
    "            saved_sample_output.assign(sample_last_output),\n",
    "            saved_sample_state.assign(sample_state)\n",
    "        ]):\n",
    "        sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_last_output, w, b))\n",
    "        \n",
    "    return (\n",
    "        optimizer,\n",
    "        loss,\n",
    "        train_prediction,\n",
    "        learning_rate,\n",
    "        \n",
    "        sample_prediction,\n",
    "        sample_input,\n",
    "        reset_sample_state,\n",
    "        \n",
    "        train_data,\n",
    "    )\n",
    "\n",
    "def run_session_for_rnn_with_lstm_cells(\n",
    "        session,\n",
    "        train_batches,\n",
    "        valid_batches,\n",
    "    \n",
    "        optimizer,\n",
    "        loss,\n",
    "        train_prediction,\n",
    "        learning_rate,\n",
    "    \n",
    "        sample_prediction,\n",
    "        sample_input,\n",
    "        reset_sample_state,\n",
    "\n",
    "        summary_frequency,\n",
    "        valid_size,\n",
    "        num_unrollings,\n",
    "    \n",
    "        train_data,\n",
    "    ):\n",
    "    \"\"\"\n",
    "    Execute tf graph in session to train and evaluate rnn with lstm cells\n",
    "    \"\"\"\n",
    "    def _run_step(step):\n",
    "        batches = train_batches.next()\n",
    "        feed_dict = {\n",
    "            train_data[i]: batches[i]\n",
    "            for i in range(num_unrollings + 1)\n",
    "        }\n",
    "        \n",
    "        if step == 0:\n",
    "            print('Train data shape', len(train_data), train_data[0].shape)\n",
    "            print('Batches example', batches)\n",
    "\n",
    "        _, l, predictions, lr = session.run(\n",
    "           [\n",
    "                optimizer, \n",
    "                loss, \n",
    "                train_prediction, \n",
    "                learning_rate\n",
    "            ], feed_dict=feed_dict)\n",
    "        \n",
    "        if step % summary_frequency == 0 and step > 0:\n",
    "            labels = np.concatenate(list(batches)[1:])\n",
    "            print('Minibatch perplexity: %.2f' % float(\n",
    "                 np.exp(logprob(predictions, labels))))\n",
    "\n",
    "        return l\n",
    "    \n",
    "    \n",
    "    tf.global_variables_initializer().run()\n",
    "    print('Initialized')\n",
    "    mean_loss = 0\n",
    "    for step in range(num_steps):\n",
    "        l = _run_step(step)\n",
    "        mean_loss += l\n",
    "    \n",
    "        if step % summary_frequency == 0 and step > 0:\n",
    "            mean_loss = mean_loss / summary_frequency\n",
    "    \n",
    "            print(\n",
    "                'Average loss at step %d: %f learning rate: %f' % (step, mean_loss, lr))\n",
    "            mean_loss = 0\n",
    "\n",
    "        if step % (summary_frequency * 10) == 0:\n",
    "            # Generate some samples.\n",
    "            print('=' * 80)\n",
    "            for _ in range(5):\n",
    "                feed = sample(random_distribution())\n",
    "                sentence = characters(feed)[0]\n",
    "                reset_sample_state.run()\n",
    "                for _ in range(79):\n",
    "                    prediction = sample_prediction.eval({sample_input: feed})\n",
    "                    feed = sample(prediction)\n",
    "                    sentence += characters(feed)[0]\n",
    "                print(sentence)\n",
    "            print('=' * 80)\n",
    "    \n",
    "            # Measure validation set perplexity.\n",
    "            reset_sample_state.run()\n",
    "            valid_logprob = 0\n",
    "            for _ in range(valid_size):\n",
    "                b = valid_batches.next()\n",
    "                predictions = sample_prediction.eval({sample_input: b[0]})\n",
    "                valid_logprob = valid_logprob + logprob(predictions, b[1])\n",
    "            print(\n",
    "                'Validation set perplexity: %.2f' % float(\n",
    "                 np.exp(valid_logprob / valid_size)))\n",
    "        \n",
    "def run_rnn_with_lstm_model(\n",
    "        vocabulary_size, \n",
    "        num_nodes,\n",
    "        batch_size,\n",
    "        num_unrollings,\n",
    "        valid_size,\n",
    "        summary_frequency,\n",
    "    \n",
    "        train_batches,\n",
    "        valid_batches,\n",
    "    \n",
    "        prepare_params_for_rnn_with_lstm,\n",
    "        cell_function,\n",
    "        get_output_for_rnn_with_lstm\n",
    "    ):\n",
    "    \"\"\"\n",
    "    Train and evaluate rnn model with lstm cells\n",
    "    \"\"\"\n",
    "    tf.reset_default_graph()\n",
    "    graph = tf.Graph()\n",
    "    with graph.as_default(), tf.device('/cpu:0'):\n",
    "        (\n",
    "            optimizer,\n",
    "            loss,\n",
    "            train_prediction,\n",
    "            learning_rate,\n",
    "        \n",
    "            sample_prediction,\n",
    "            sample_input,\n",
    "            reset_sample_state,\n",
    "\n",
    "            train_data,           \n",
    "        ) = generate_rnn_graph_with_lstm(\n",
    "            vocabulary_size, \n",
    "            num_nodes,\n",
    "            batch_size,\n",
    "            num_unrollings,\n",
    "\n",
    "            prepare_params_for_rnn_with_lstm,\n",
    "            cell_function,\n",
    "            get_output_for_rnn_with_lstm\n",
    "        )\n",
    "        \n",
    "    with tf.Session(graph=graph) as session:\n",
    "        run_session_for_rnn_with_lstm_cells(\n",
    "            session,\n",
    "            train_batches,\n",
    "            valid_batches,\n",
    "    \n",
    "            optimizer,\n",
    "            loss,\n",
    "            train_prediction,\n",
    "            learning_rate,\n",
    "    \n",
    "            sample_prediction,\n",
    "            sample_input,\n",
    "            reset_sample_state,\n",
    "\n",
    "            summary_frequency,\n",
    "            valid_size,\n",
    "            num_unrollings,\n",
    "            train_data,)\n",
    "        \n",
    "run_rnn_with_lstm_model(\n",
    "        vocabulary_size, \n",
    "        num_nodes,\n",
    "        batch_size,\n",
    "        num_unrollings,\n",
    "        valid_size,\n",
    "        summary_frequency,\n",
    "    \n",
    "        BatchGenerator(train_text, batch_size, num_unrollings),\n",
    "        BatchGenerator(valid_text, 1, 1),\n",
    "\n",
    "        prepare_params_for_rnn_with_lstm,\n",
    "        lstm_cell_with_diff_params_per_gate,\n",
    "        get_output_for_rnn_with_lstm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pl4vtmFfa5nn"
   },
   "source": [
    "---\n",
    "Problem 1\n",
    "---------\n",
    "\n",
    "You might have noticed that the definition of the LSTM cell involves 4 matrix multiplications with the input, and 4 matrix multiplications with the output. Simplify the expression by using a single matrix multiply for each, and variables that are 4 times larger.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train inputs shape 10 (64, 27)\n",
      "Train labels shape 10 (64, 27)\n",
      "Outputs shape (640, 64)\n",
      "Last output shape (64, 64)\n",
      "State shape (64, 64)\n",
      "Train prediction shape (640, 27)\n",
      "Initialized\n",
      "Train data shape 11 (64, 27)\n",
      "Batches example [array([[0., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.],\n",
      "       ...,\n",
      "       [0., 1., 0., ..., 0., 0., 0.],\n",
      "       [0., 1., 0., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.]]), array([[0., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.],\n",
      "       ...,\n",
      "       [0., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.]]), array([[0., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.],\n",
      "       ...,\n",
      "       [0., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.],\n",
      "       [1., 0., 0., ..., 0., 0., 0.]]), array([[1., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.],\n",
      "       ...,\n",
      "       [0., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.]]), array([[0., 1., 0., ..., 0., 0., 0.],\n",
      "       [1., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.],\n",
      "       ...,\n",
      "       [0., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 1., 0., ..., 0., 0., 0.],\n",
      "       [0., 1., 0., ..., 0., 0., 0.]]), array([[0., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 1., 0., ..., 0., 0., 0.],\n",
      "       ...,\n",
      "       [0., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.]]), array([[0., 1., 0., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.],\n",
      "       [1., 0., 0., ..., 0., 0., 0.],\n",
      "       ...,\n",
      "       [1., 0., 0., ..., 0., 0., 0.],\n",
      "       [1., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.]]), array([[0., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 1., 0., ..., 0., 0., 0.],\n",
      "       ...,\n",
      "       [0., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.],\n",
      "       [1., 0., 0., ..., 0., 0., 0.]]), array([[0., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.],\n",
      "       ...,\n",
      "       [0., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.]]), array([[0., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.],\n",
      "       ...,\n",
      "       [0., 0., 0., ..., 0., 0., 0.],\n",
      "       [1., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 1., 0., ..., 0., 0., 0.]]), array([[0., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 1., 0., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.],\n",
      "       ...,\n",
      "       [0., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.]])]\n",
      "================================================================================\n",
      "xmk mzluf wgckaliittlovevue nnj wtrsaxxoed pswelq tlnr nloxrbefioa koha uxatwqa \n",
      "ppcesemm biasgi  uowfevzwodr mgfyes eie  d m nn moxbsmz e umjga ho etsocaoazajrn\n",
      "naedrxq vw i raytiisaut onlahida g axat scrydweni pjvswddrblhinpbqvmfwrtjy  gklk\n",
      "xhauamkybtskhbc x  ib eokqnrcnxatwrashyr sergqjo atogmjiopurjmpipirkrpnqj   lyc \n",
      "feld k wvzwapeomrgit eicvf edmw uanxlceug lmippdn elfqss fxigrne  nwliwjn v woew\n",
      "================================================================================\n",
      "Validation set perplexity: 20.07\n",
      "Minibatch perplexity: 10.04\n",
      "Average loss at step 100: 2.632888 learning rate: 1.000000\n",
      "Minibatch perplexity: 7.73\n",
      "Average loss at step 200: 2.236882 learning rate: 1.000000\n",
      "Minibatch perplexity: 7.53\n",
      "Average loss at step 300: 2.088388 learning rate: 1.000000\n",
      "Minibatch perplexity: 7.01\n",
      "Average loss at step 400: 1.996107 learning rate: 1.000000\n",
      "Minibatch perplexity: 7.30\n",
      "Average loss at step 500: 1.933050 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.49\n",
      "Average loss at step 600: 1.905979 learning rate: 1.000000\n",
      "Minibatch perplexity: 6.64\n",
      "Average loss at step 700: 1.854423 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.78\n",
      "Average loss at step 800: 1.815982 learning rate: 1.000000\n",
      "Minibatch perplexity: 6.03\n",
      "Average loss at step 900: 1.826228 learning rate: 1.000000\n",
      "Minibatch perplexity: 6.27\n",
      "Average loss at step 1000: 1.823723 learning rate: 1.000000\n",
      "================================================================================\n",
      "e nitter is the awditwate the night umers a could accrever of the manifite tunca\n",
      "w the duze the tink reveraskhy sever of clasitutor but a toal the litire in inga\n",
      "x threict licht assy jeatoe of zero from the five sollen three stovity him divel\n",
      "her diver as the luremy beade assatia reto the eteaciss two two cold severte the\n",
      "oph when eventics state is inielfa moter the gor the lasen an oppases in thearze\n",
      "================================================================================\n",
      "Validation set perplexity: 6.16\n",
      "Minibatch perplexity: 5.86\n",
      "Average loss at step 1100: 1.773290 learning rate: 1.000000\n",
      "Minibatch perplexity: 6.10\n",
      "Average loss at step 1200: 1.752956 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.21\n",
      "Average loss at step 1300: 1.730475 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.98\n",
      "Average loss at step 1400: 1.745548 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.75\n",
      "Average loss at step 1500: 1.737696 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.89\n",
      "Average loss at step 1600: 1.744467 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.53\n",
      "Average loss at step 1700: 1.715185 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.65\n",
      "Average loss at step 1800: 1.676741 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.05\n",
      "Average loss at step 1900: 1.647859 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.22\n",
      "Average loss at step 2000: 1.698069 learning rate: 1.000000\n",
      "================================================================================\n",
      "kanarthe ses bow sueptiis or gones when in mansmandimal beging the two concressh\n",
      "y the pap zero singer a seash regenfiles one zero zero four one foun enstituats \n",
      " opcorda hix englinn largeer d rovilal generall gromt insturts of by a rumeg and\n",
      "quesss dunger prest carlish recent of a one nine three zero zero zero zero one n\n",
      "intally slewen and kimani oundai awoon on mostles signial we to solea selences n\n",
      "================================================================================\n",
      "Validation set perplexity: 5.46\n",
      "Minibatch perplexity: 5.40\n",
      "Average loss at step 2100: 1.689059 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.03\n",
      "Average loss at step 2200: 1.675781 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.12\n",
      "Average loss at step 2300: 1.644596 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.60\n",
      "Average loss at step 2400: 1.657020 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.38\n",
      "Average loss at step 2500: 1.676995 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.28\n",
      "Average loss at step 2600: 1.652263 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.33\n",
      "Average loss at step 2700: 1.655422 learning rate: 1.000000\n",
      "Minibatch perplexity: 6.10\n",
      "Average loss at step 2800: 1.645458 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.97\n",
      "Average loss at step 2900: 1.649813 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.87\n",
      "Average loss at step 3000: 1.646793 learning rate: 1.000000\n",
      "================================================================================\n",
      "lio severy and ordentory to naturation of werry knower monar slame ple defords b\n",
      "s his dially after to geneine the by town the standsent to there and manarbed fa\n",
      "was see one nine seven photomom one nine six nine contaitity are if be prorde ha\n",
      "f two mebire one nine sevenen tach are of abissim nedrely twle interices one nin\n",
      "overina hapa j shirth one six nine eight zero to the vonter hellow throg the gou\n",
      "================================================================================\n",
      "Validation set perplexity: 4.83\n",
      "Minibatch perplexity: 4.59\n",
      "Average loss at step 3100: 1.624214 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.22\n",
      "Average loss at step 3200: 1.648692 learning rate: 1.000000\n"
     ]
    }
   ],
   "source": [
    "# Calculate LSTM value with 4 times less matmuls\n",
    "\n",
    "GATES_AND_CELLS_NUMBER = 4 # 3 gates and 1 memeory cell\n",
    "\n",
    "def prepare_params_for_rnn_with_fast_lstm(\n",
    "       vocabulary_size, \n",
    "       num_nodes,\n",
    "    ):\n",
    "    \"\"\"\n",
    "    Prepare params for rnn with lstm\n",
    "    \"\"\"\n",
    "    # Parameters:\n",
    "    gates_input = tf.Variable(\n",
    "        tf.truncated_normal(\n",
    "            [\n",
    "                vocabulary_size, \n",
    "                num_nodes * GATES_AND_CELLS_NUMBER # input gate + forget gate + memory cell + output gate\n",
    "            ], -0.1, 0.1))\n",
    "    gates_prev_output = tf.Variable(\n",
    "        tf.truncated_normal(\n",
    "            [\n",
    "                num_nodes, \n",
    "                num_nodes * GATES_AND_CELLS_NUMBER # input gate + forget gate + memory cell + output gate\n",
    "            ], -0.1, 0.1))\n",
    "\n",
    "    input_gate_bias = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    forget_gate_bias = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    memory_cell_bias = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    output_gate_bias = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    \n",
    "    return (\n",
    "        gates_input,\n",
    "        gates_prev_output,\n",
    "        \n",
    "        input_gate_bias,\n",
    "        forget_gate_bias,\n",
    "        memory_cell_bias,\n",
    "        output_gate_bias)\n",
    "\n",
    "\n",
    "def fast_lstm_cell(\n",
    "        i, o, state,\n",
    "\n",
    "        gates_input,\n",
    "        gates_prev_output,\n",
    "        \n",
    "        input_gate_bias,\n",
    "        forget_gate_bias,\n",
    "        memory_cell_bias,\n",
    "        output_gate_bias):\n",
    "    \"\"\"Create a LSTM cell using only 2 matmuls: for input and output\"\"\"\n",
    "\n",
    "    i = tf.matmul(i, gates_input)\n",
    "    o = tf.matmul(o, gates_prev_output)\n",
    "    (\n",
    "        input_gate_i,\n",
    "        forget_gate_i,\n",
    "        memory_cell_i,\n",
    "        output_gate_i\n",
    "    ) = tf.split(i, GATES_AND_CELLS_NUMBER, axis=1)\n",
    "    (\n",
    "        input_gate_o,\n",
    "        forget_gate_o,\n",
    "        memory_cell_o,\n",
    "        output_gate_o\n",
    "    ) = tf.split(o, GATES_AND_CELLS_NUMBER, axis=1)\n",
    "\n",
    "    input_gate = tf.sigmoid(\n",
    "        input_gate_i + \\\n",
    "        input_gate_o + \\\n",
    "        input_gate_bias)\n",
    "    forget_gate = tf.sigmoid(\n",
    "        forget_gate_i + \\\n",
    "        forget_gate_o + forget_gate_bias)\n",
    "    update = \\\n",
    "        memory_cell_i + \\\n",
    "        memory_cell_o + \\\n",
    "        memory_cell_bias\n",
    "    state = forget_gate * state + input_gate * tf.tanh(update)\n",
    "    output_gate = tf.sigmoid(\n",
    "        output_gate_i + \\\n",
    "        output_gate_o + \\\n",
    "        output_gate_bias)\n",
    "    return output_gate * tf.tanh(state), state\n",
    "\n",
    "run_rnn_with_lstm_model(\n",
    "        vocabulary_size, \n",
    "        num_nodes,\n",
    "        batch_size,\n",
    "        num_unrollings,\n",
    "        valid_size,\n",
    "        summary_frequency,\n",
    "    \n",
    "        BatchGenerator(train_text, batch_size, num_unrollings),\n",
    "        BatchGenerator(valid_text, 1, 1),\n",
    "\n",
    "        prepare_params_for_rnn_with_fast_lstm,\n",
    "        fast_lstm_cell,\n",
    "        get_output_for_rnn_with_lstm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4eErTCTybtph"
   },
   "source": [
    "---\n",
    "Problem 2\n",
    "---------\n",
    "\n",
    "We want to train a LSTM over bigrams, that is pairs of consecutive characters like 'ab' instead of single characters like 'a'. Since the number of possible bigrams is large, feeding them directly to the LSTM using 1-hot encodings will lead to a very sparse representation that is very wasteful computationally.\n",
    "\n",
    "a- Introduce an embedding lookup on the inputs, and feed the embeddings to the LSTM cell instead of the inputs themselves.\n",
    "\n",
    "b- Write a bigram-based LSTM, modeled on the character LSTM above.\n",
    "\n",
    "c- Introduce Dropout. For best practices on how to use Dropout in LSTMs, refer to this [article](http://arxiv.org/abs/1409.2329).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Y5tapX3kpcqZ"
   },
   "source": [
    "---\n",
    "Problem 3\n",
    "---------\n",
    "\n",
    "(difficult!)\n",
    "\n",
    "Write a sequence-to-sequence LSTM which mirrors all the words in a sentence. For example, if your input is:\n",
    "\n",
    "    the quick brown fox\n",
    "    \n",
    "the model should attempt to output:\n",
    "\n",
    "    eht kciuq nworb xof\n",
    "    \n",
    "Refer to the lecture on how to put together a sequence-to-sequence model, as well as [this article](http://arxiv.org/abs/1409.3215) for best practices.\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "default_view": {},
   "name": "6_lstm.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
